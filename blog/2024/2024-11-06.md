---
slug: hariprasadgajurel
title: Fully connected neural network for 2D transient heat equation by using PINN.
authors:
  hpg1164
tags: [Machine Learning, python,PINNS, PDE, tutorial]
---
This article highlights the shift from isogeometric analysis and finite difference methods to Physics-Informed Neural Networks (PINNs) for solving PDEs. Introduced by Prof. Karniadakis in 2019, PINNs approximate solutions by embedding neural networks directly into PDE formulations, using residuals of the equation and boundary conditions as the loss function.

PINNs have grown rapidly, offering flexibility for complex and data-driven problems, complementing tools like ANSYS. Their ease of use on platforms like Colab makes them a modern, accessible alternative to traditional solvers for diverse applications in mechanics, geology, and medicine.

## Problem statement
Traditional numerical methods, susch as the finite element methosd , are widely used for solving PDEs. However, these methods often suffer from high computational costs ,espically for complex , high-dimensional problems . Physics Informed Neural Network(PINN) offer an efficent alternative by embedding the govering data-driven machine learning to approximate solutions while incorporating physical constraints, making them suitable for application in engineering,fluid dynamics and heat transfer.



The goal was to solve a 2D transient heat conduction problem using PINNs and evaluate the performance impact of various neural network architecture on accurcy and computational efficiency.  
![Fig:2D Transient heat conduction problem](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/graph/problem%20statement.png)

# Boundary Conditions
bottom wall = T(x,0) = 273K,
top wall = T(x,1) = 400k,
left wall = T(0,y) = 300k,
right wall = T(1,y) = 300k,

when normalized:

bottom wall = T(x,0) = 273/400 = 0.6825,
top wall = T(x,1) = 400/400 = 1,
left wall = T(0,y) = 300/400 = 0.75.
right wall = T(1,y) = 300/400 = 0.75



## Ansys setup
The problem is setup first solved using an Ansys solver to get ground truth to compare the output to PINN.
<li>Solver : Static structural</li>
<li>Mesh Element Type : Rectangular</li>
<li>Element Order : Quardic</li>
<li>Element Size : 0.01m.</li>
<li>Number of node : 30401 </li>
<li>Number of Elements : 10000 </li>

![](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/graph/Ansys%20domain.png)

![](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/graph/ansys_boundary%20.png)


## Finite difference method
The finite difference method(FDM)is a numerical technique used to approximate solution to differential equations by replacing derivatives with different equations.
<ul>
<li> Sets up numerical grid with 21 x 21 points</li>
<li> Creating a uniform spatial discritization</li>
<li> Each interior is upload using the explict finite difference approximation.</li>
</ul>
![](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/graph/fdmanalysis.png) 




## Physics informed neural network(PINN)

  We present the PINN library with the following features:
<ul>
    <li>It is compatiable with google colab which allows automatic executation in a cloud environment.</li>
    <li>It supports two-dimensional problems defined on a rectangular domain.</li>
    <li> It provide a simple interface for the definition of the residual loss, boundary condition, and initial loss, together with their weights. </li>
    <li>It allows for customizing the number of layers and neurons per layer, as well as for arbitary activation functions.</li>
    <li>It supports Neumann and Dirichlet boundary conditions.</li>
    <li>The learning rate and number of epochs are available as parameters.</li> 
    
</ul>

## Installing pytorch library

```bash
 ! pip install torch  torchvision
```

[Note: If you are getting errors while installing , you can alternatively create a virtual environment in Python. Go to any command line, command prompt, or bash or you can do it in google colab:]


```bash
 ! pip install pyDOE
```







### `getData()`
 Function getData() that generates and prepares data points for training a Physics-Informed Neural Network (PINN). In the context of solving partial differential equations (PDEs), PINNs leverage physical laws in the form of PDEs as part of the training process, combining traditional data-driven learning with physics-based loss terms.



```python
left_wall = [x_min, y_min, t_min] + [0.0, y_max-y_min, t_max] * lhs(3, N_b)
right_wall = [x_max, y_min, t_min] + [0.0, y_max-y_min, t_max] * lhs(3, N_b)
bottom_wall = [x_min, y_min, t_min] + [x_max-x_min, 0.0, t_max] * lhs(3, N_b)
top_wall = [x_min, y_max, t_min] + [x_max-x_min, 0.0, t_max] * lhs(3, N_b)

```

   
## Fully connected Neural Network Analysis(FCN)
Architecture:
![](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/fcnarc.png)

###  Explanation of the Architecture
This architecture represents a Physiscs -Informed Neural Network (PINN) designed to solve a Partial Differential equation 
#### Input layer:
The network takes sparial and temporal variables ,such as x, y, t as inputs. These variables represent the domain where the PDE is defined.


#### Output layer:
The output layer provides the predicted solution f(x, y ,t) of the PDE.

#### Residual Function 
The residual function is defined as:
R = f(x, y ,t) -g(t),
which represent the error in satisfying the PDE and the residual R is used to enforce the PDE constraints during training.


#### Loss Function

The total loss is the combination of multiple terms:

Loss = MSE(T) + MSE(BC) + MSE(IC)+MSE(R),

Where,
MSE(T): Mean square error for the temporal data
MSE(BC) : Mean Square error for the boundary conditions
MSE(IC) :Mean Square error for the initial conditions.
MSE(R) : Mean sqaure error for residual





### `tranning():`

```python
if __name__ == "__main__":

    pinn = PINN()
    start_time = time.time()

    for i in range(8000):
        pinn.closure()
        pinn.adam.step()
    pinn.lbfgs.step(pinn.closure)

    print("--- %s seconds ---" % (time.time() - start_time))
    print(f'-- {(time.time() - start_time)/60} mins --')

    # Save model state
    torch.save(pinn.net.state_dict(), "/content/Param.pt")

    # Plot loss curve
    plotLoss(pinn.losses, "/content/LossCurve.png", ["BC", "IC", "PDE"])
    ```

   The training loop aims to minimize the combined losses (BC, IC, PDE) using two optimization strategies:
Adam optimizer for fast, adaptive updates in the early stages.
L-BFGS optimizer for a more precise, second-order optimization step at the end.
The model's performance is logged, and results are saved for later analysis or further use.

##  Loss analysis in the differnt architecture.

 ### 4x50 architecture in FCN


![](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/4x50ansys.png)
![](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/4x50pinn.png)
![](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/4x50res.png)

![](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/output4x50FCN.png)

The 4 x 50 architecture balance complexity (4 layers) and computational efficiency(50 neurons per layer),which is critical for solving  PDEs without overfitting. We can see the the loss converges but residuals remain high in specific regions , targeted refinements.
### 5x100 architecture in FCN

![](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/ansys5x100.png)
![](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/5x100pinn.png)

![](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/res5x100.png)

![](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/graph/output5x100FCN.png)


5x100 model likely improves accuracy but requires more data and computational resoureces  and in te residual analysis there is need for adaotive sampling or loss weighting in challenginf regions even with large networks.

 ### 6x200 architecture in FCN

![](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/ansys5x100.png)
![](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/6x200pinn.png)

![](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/res6x200.png)
![](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/graph/output6x200FCN.png)


6x200 Accuracy improves with network size, but computational costs grow exponentially and it reduces global error but may fail in localized regions.


Total loss analysis after tranning the all neural architecture:

![](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/graph/lossFCN.png)


From the above graph we can conclude that 5x100 network offer the best balance of accuracy and efficiency for moderately complex PDEs and 6x100 architecture could be justified for highly nonlinear .













#### Animation :

![](https://github.com/hpg1164/solving-heat-equation-with-PINN/raw/main/5x100%20archi.gif)

This animation shows the temperature distribution on the rectagular domain with time.

### Conclusion

Here the process of selecting neural network architectures for solving partial differential eqwuations is largely on a hit -and trial approch.Insted, practitioners rely on experimnentation to identify architectures that yield better performance for a given problem .This underscores the importances of systematically testing various configurations ans analzing their impact on model accuracy and convergences.





